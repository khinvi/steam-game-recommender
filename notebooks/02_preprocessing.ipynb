{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create User-Item Interaction Matrix\n",
    "user_ids = reviews_df['user_id'].unique()\n",
    "item_ids = reviews_df['item_id'].unique()\n",
    "\n",
    "user_to_idx = {user: i for i, user in enumerate(user_ids)}\n",
    "item_to_idx = {item: i for i, item in enumerate(item_ids)}\n",
    "idx_to_user = {i: user for user, i in user_to_idx.items()}\n",
    "idx_to_item = {i: item for item, i in item_to_idx.items()}\n",
    "\n",
    "# Create interaction matrix using normalized playtime\n",
    "interaction_matrix = reviews_df.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='normalized_playtime',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "print(f\"Sparsity: {100.0 * (1 - np.count_nonzero(interaction_matrix) / interaction_matrix.size):.2f}%\")\n",
    "\n",
    "# Convert to sparse matrix for efficiency\n",
    "sparse_interaction_matrix = csr_matrix(interaction_matrix.values)\n",
    "\n",
    "# Create binary interaction matrix (played or not played)\n",
    "binary_interaction_matrix = (interaction_matrix > 0).astype(int)\n",
    "sparse_binary_matrix = csr_matrix(binary_interaction_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a sample of the interaction matrix to understand the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset for visualization\n",
    "n_sample = 50\n",
    "sample_users = np.random.choice(interaction_matrix.index, min(n_sample, len(interaction_matrix.index)), replace=False)\n",
    "sample_items = np.random.choice(interaction_matrix.columns, min(n_sample, len(interaction_matrix.columns)), replace=False)\n",
    "\n",
    "sample_matrix = interaction_matrix.loc[sample_users, sample_items]\n",
    "\n",
    "# Visualize the sample\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(sample_matrix, cmap='viridis')\n",
    "plt.title(f'Sample of User-Item Interaction Matrix ({n_sample}x{n_sample})')\n",
    "plt.xlabel('Item ID')\n",
    "plt.ylabel('User ID')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Sets\n",
    "\n",
    "To evaluate our recommendation models, we'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for train-test split at the user level\n",
    "def train_test_split_leave_n_out(df, n_test_items=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split interactions into train and test sets using leave-n-out strategy per user.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing user-item interactions\n",
    "        n_test_items: Number of items to leave out for each user\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_df: DataFrame with training interactions\n",
    "        test_df: DataFrame with test interactions\n",
    "    \"\"\"\n",
    "    train_df = pd.DataFrame(columns=df.columns)\n",
    "    test_df = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # Group by user_id\n",
    "    grouped = df.groupby('user_id')\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    for user_id, user_data in tqdm(grouped, desc=\"Splitting data\"):\n",
    "        # Ensure user has enough items to split\n",
    "        n_items = len(user_data)\n",
    "        if n_items <= n_test_items:\n",
    "            # If user doesn't have enough items, put all in training\n",
    "            train_df = pd.concat([train_df, user_data])\n",
    "            continue\n",
    "        \n",
    "        # Randomly select test items\n",
    "        test_indices = np.random.choice(user_data.index, n_test_items, replace=False)\n",
    "        test_items = user_data.loc[test_indices]\n",
    "        train_items = user_data.drop(test_indices)\n",
    "        \n",
    "        # Add to respective dataframes\n",
    "        train_df = pd.concat([train_df, train_items])\n",
    "        test_df = pd.concat([test_df, test_items])\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Apply the split\n",
    "train_df, test_df = train_test_split_leave_n_out(reviews_df, n_test_items=2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} interactions, {train_df['user_id'].nunique()} users, {train_df['item_id'].nunique()} items\")\n",
    "print(f\"Test set: {len(test_df)} interactions, {test_df['user_id'].nunique()} users, {test_df['item_id'].nunique()} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create interaction matrices for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction matrices for training data\n",
    "train_matrix = train_df.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='normalized_playtime',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Convert to sparse matrix for efficiency\n",
    "sparse_train_matrix = csr_matrix(train_matrix.values)\n",
    "\n",
    "# Create binary interaction matrix for training data\n",
    "binary_train_matrix = (train_matrix > 0).astype(int)\n",
    "sparse_binary_train_matrix = csr_matrix(binary_train_matrix.values)\n",
    "\n",
    "print(f\"Training interaction matrix shape: {train_matrix.shape}\")\n",
    "print(f\"Training matrix sparsity: {100.0 * (1 - np.count_nonzero(train_matrix) / train_matrix.size):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "\n",
    "Now let's save all the processed data for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "processed_dir = '../data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save DataFrames\n",
    "train_df.to_csv(f'{processed_dir}/train_interactions.csv', index=False)\n",
    "test_df.to_csv(f'{processed_dir}/test_interactions.csv', index=False)\n",
    "items_df.to_csv(f'{processed_dir}/items.csv', index=False)\n",
    "\n",
    "# Save interaction matrices\n",
    "train_matrix.to_csv(f'{processed_dir}/train_matrix.csv')\n",
    "binary_train_matrix.to_csv(f'{processed_dir}/binary_train_matrix.csv')\n",
    "\n",
    "# Save ID mappings\n",
    "id_mappings = {\n",
    "    'user_to_idx': user_to_idx,\n",
    "    'item_to_idx': item_to_idx,\n",
    "    'idx_to_user': idx_to_user,\n",
    "    'idx_to_item': idx_to_item\n",
    "}\n",
    "\n",
    "with open(f'{processed_dir}/id_mappings.pkl', 'wb') as f:\n",
    "    pickle.dump(id_mappings, f)\n",
    "\n",
    "# Save sparse matrices\n",
    "sparse_matrices = {\n",
    "    'sparse_train_matrix': sparse_train_matrix,\n",
    "    'sparse_binary_train_matrix': sparse_binary_train_matrix\n",
    "}\n",
    "\n",
    "with open(f'{processed_dir}/sparse_matrices.pkl', 'wb') as f:\n",
    "    pickle.dump(sparse_matrices, f)\n",
    "    \n",
    "print(f\"Processed data saved to {processed_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Summary\n",
    "\n",
    "Let's summarize the preprocessing steps we've performed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've completed the following preprocessing steps:\n",
    "\n",
    "1. **Data Cleaning and Initial Transformations**\n",
    "   - Selected essential columns: user_id, item_id, playtime_forever, item_name, and genres (when available)\n",
    "   - Handled missing values by dropping rows with missing user or item IDs\n",
    "   - Filled missing playtime values with 0\n",
    "\n",
    "2. **Outlier Handling**\n",
    "   - Identified outliers in playtime using IQR method\n",
    "   - Capped extremely high playtime values to reduce their impact on the models\n",
    "\n",
    "3. **Filtering**\n",
    "   - Applied minimum thresholds for user activity (>= 5 games) and game popularity (>= 10 users)\n",
    "   - Filtered out users and games with insufficient data for reliable recommendations\n",
    "\n",
    "4. **Playtime Normalization**\n",
    "   - Transformed playtime values to a 0-5 scale using a logarithmic transformation\n",
    "   - This preserves the relative differences in playtime while making values more comparable\n",
    "\n",
    "5. **Interaction Matrix Creation**\n",
    "   - Created user-item interaction matrices with normalized playtime values\n",
    "   - Generated binary versions of matrices (played/not played)\n",
    "   - Converted to sparse representations for efficient computation\n",
    "\n",
    "6. **Train-Test Splitting**\n",
    "   - Used leave-n-out strategy, holding out 2 items per user for testing\n",
    "   - Created train and test DataFrames and matrices\n",
    "\n",
    "7. **Data Saving**\n",
    "   - Saved all processed data for use in model development\n",
    "\n",
    "The processed data maintains important features from the original dataset while addressing issues like sparsity, outliers, and data format that could affect model performance. The normalization step is particularly important as it transforms playtime into a more interpretable scale while preserving the signal of user preference.\n",
    "\n",
    "In the next notebook, we'll implement and evaluate baseline recommendation models using this processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final statistics for the processed data\n",
    "print(\"Final Dataset Statistics\")\n",
    "print(\"=========================\\n\")\n",
    "print(f\"Number of users: {train_df['user_id'].nunique()}\")\n",
    "print(f\"Number of items: {train_df['item_id'].nunique()}\")\n",
    "print(f\"Number of interactions: {len(train_df)}\")\n",
    "print(f\"Data density: {100.0 * np.count_nonzero(train_matrix) / train_matrix.size:.4f}%\")\n",
    "print(f\"Average items per user: {len(train_df) / train_df['user_id'].nunique():.2f}\")\n",
    "print(f\"Average users per item: {len(train_df) / train_df['item_id'].nunique():.2f}\")\n",
    "print(f\"Normalized playtime range: {train_df['normalized_playtime'].min():.2f} - {train_df['normalized_playtime'].max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
