{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam Game Recommender System - Data Preprocessing\n",
    "\n",
    "This notebook handles the preprocessing steps for the Steam dataset to prepare it for model development. We'll focus on:\n",
    "\n",
    "1. Loading the raw data\n",
    "2. Cleaning and transforming the data\n",
    "3. Filtering users and games based on activity thresholds\n",
    "4. Normalizing playtime information\n",
    "5. Creating the user-item interaction matrix\n",
    "6. Splitting data into training and testing sets\n",
    "7. Saving the processed data for model training\n",
    "\n",
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Create data directories if they don't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reuse our loading function from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample_size=None):\n",
    "    \"\"\"\n",
    "    Load the Steam dataset files (reviews and items).\n",
    "    \n",
    "    Args:\n",
    "        sample_size: Number of reviews to sample (for faster processing)\n",
    "        \n",
    "    Returns:\n",
    "        reviews_df: DataFrame containing user reviews/interactions\n",
    "        items_df: DataFrame containing game metadata\n",
    "    \"\"\"\n",
    "    # Define file paths - try multiple possible locations\n",
    "    possible_paths = [\n",
    "        ('../data/reviews_v2.json.gz', '../data/items_v2.json.gz'),\n",
    "        ('../data/steam_reviews.json.gz', '../data/steam_games.json.gz'),\n",
    "        ('data/reviews_v2.json.gz', 'data/items_v2.json.gz'),\n",
    "        ('data/steam_reviews.json.gz', 'data/steam_games.json.gz')\n",
    "    ]\n",
    "    \n",
    "    # Try to find the data files\n",
    "    reviews_path, items_path = None, None\n",
    "    for r_path, i_path in possible_paths:\n",
    "        if os.path.exists(r_path) and os.path.exists(i_path):\n",
    "            reviews_path, items_path = r_path, i_path\n",
    "            break\n",
    "    \n",
    "    if reviews_path is None:\n",
    "        raise FileNotFoundError(\"Could not find data files. Please check paths.\")\n",
    "        \n",
    "    print(f\"Loading reviews from {reviews_path}\")\n",
    "    print(f\"Loading items from {items_path}\")\n",
    "    \n",
    "    # Load reviews data\n",
    "    reviews = []\n",
    "    with gzip.open(reviews_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=\"Loading reviews\")):\n",
    "            if sample_size is not None and i >= sample_size:\n",
    "                break\n",
    "            try:\n",
    "                reviews.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Load items data\n",
    "    items = []\n",
    "    with gzip.open(items_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading items\"):\n",
    "            try:\n",
    "                items.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    reviews_df = pd.DataFrame(reviews)\n",
    "    items_df = pd.DataFrame(items)\n",
    "    \n",
    "    return reviews_df, items_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data. For preprocessing, we can use a larger sample size than we used for initial exploration, but still limit it to make processing manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a larger sample of the data for preprocessing\n",
    "# Set to None to load the full dataset (may take significant time)\n",
    "SAMPLE_SIZE = 200000\n",
    "\n",
    "try:\n",
    "    reviews_df, items_df = load_data(sample_size=SAMPLE_SIZE)\n",
    "    print(f\"Loaded {len(reviews_df)} reviews and {len(items_df)} items.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Creating sample data for demonstration purposes...\")\n",
    "    \n",
    "    # Create synthetic data for demonstration if real data is unavailable\n",
    "    import random\n",
    "    \n",
    "    # Create sample reviews\n",
    "    n_users = 2000\n",
    "    n_games = 1000\n",
    "    n_reviews = 50000\n",
    "    \n",
    "    user_ids = [f\"user_{i}\" for i in range(n_users)]\n",
    "    game_ids = [f\"game_{i}\" for i in range(n_games)]\n",
    "    \n",
    "    reviews = []\n",
    "    for _ in range(n_reviews):\n",
    "        user_id = random.choice(user_ids)\n",
    "        game_id = random.choice(game_ids)\n",
    "        playtime = random.randint(0, 1000) if random.random() > 0.2 else 0\n",
    "        reviews.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id': game_id,\n",
    "            'playtime_forever': playtime,\n",
    "            'playtime_2weeks': min(playtime, random.randint(0, 20)) if playtime > 0 else 0\n",
    "        })\n",
    "    \n",
    "    # Create sample games\n",
    "    game_names = [f\"Game Title {i}\" for i in range(n_games)]\n",
    "    genres = ['Action', 'Adventure', 'RPG', 'Strategy', 'Simulation', 'Sports', 'Racing', 'Puzzle']\n",
    "    \n",
    "    items = []\n",
    "    for i in range(n_games):\n",
    "        n_genres = random.randint(1, 3)\n",
    "        game_genres = random.sample(genres, n_genres)\n",
    "        items.append({\n",
    "            'item_id': game_ids[i],\n",
    "            'item_name': game_names[i],\n",
    "            'genres': game_genres\n",
    "        })\n",
    "    \n",
    "    reviews_df = pd.DataFrame(reviews)\n",
    "    items_df = pd.DataFrame(items)\n",
    "    \n",
    "    print(f\"Created {len(reviews_df)} synthetic reviews and {len(items_df)} synthetic items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Initial Transformations\n",
    "\n",
    "Let's start by examining and cleaning the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the dataframes\n",
    "print(\"Reviews DataFrame:\")\n",
    "print(reviews_df.columns.tolist())\n",
    "print(\"\\nItems DataFrame:\")\n",
    "print(items_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in reviews:\")\n",
    "print(reviews_df.isnull().sum())\n",
    "print(\"\\nMissing values in items:\")\n",
    "print(items_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the essential columns for our recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only essential columns for the reviews DataFrame\n",
    "essential_review_columns = ['user_id', 'item_id', 'playtime_forever']\n",
    "if 'playtime_2weeks' in reviews_df.columns:\n",
    "    essential_review_columns.append('playtime_2weeks')\n",
    "\n",
    "# Check if all essential columns exist\n",
    "missing_columns = [col for col in essential_review_columns if col not in reviews_df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Warning: Missing essential columns in reviews_df: {missing_columns}\")\n",
    "    # Try to identify alternative column names\n",
    "    if 'user_id' in missing_columns and 'steamid' in reviews_df.columns:\n",
    "        reviews_df['user_id'] = reviews_df['steamid']\n",
    "        missing_columns.remove('user_id')\n",
    "    if 'item_id' in missing_columns and 'product_id' in reviews_df.columns:\n",
    "        reviews_df['item_id'] = reviews_df['product_id']\n",
    "        missing_columns.remove('item_id')\n",
    "    if 'playtime_forever' in missing_columns and 'hours' in reviews_df.columns:\n",
    "        reviews_df['playtime_forever'] = reviews_df['hours'] * 60  # Convert hours to minutes\n",
    "        missing_columns.remove('playtime_forever')\n",
    "    \n",
    "    # If still missing essential columns, create fallback data\n",
    "    if missing_columns:\n",
    "        print(f\"Still missing essential columns: {missing_columns}. Creating fallback data.\")\n",
    "        # Add missing columns with default values\n",
    "        for col in missing_columns:\n",
    "            if col == 'playtime_forever':\n",
    "                reviews_df[col] = np.random.randint(0, 1000, size=len(reviews_df))\n",
    "            elif col == 'playtime_2weeks':\n",
    "                # Not essential, can skip\n",
    "                pass\n",
    "            else:\n",
    "                reviews_df[col] = [f\"{col}_{i}\" for i in range(len(reviews_df))]\n",
    "\n",
    "# Keep only available essential columns\n",
    "essential_review_columns = [col for col in essential_review_columns if col in reviews_df.columns]\n",
    "reviews_df = reviews_df[essential_review_columns]\n",
    "\n",
    "# Keep only essential columns for the items DataFrame\n",
    "essential_item_columns = ['item_id', 'item_name']\n",
    "if 'genres' in items_df.columns:\n",
    "    essential_item_columns.append('genres')\n",
    "\n",
    "# Check if all essential columns exist\n",
    "missing_columns = [col for col in essential_item_columns if col not in items_df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Warning: Missing essential columns in items_df: {missing_columns}\")\n",
    "    # Try to identify alternative column names\n",
    "    if 'item_id' in missing_columns and 'product_id' in items_df.columns:\n",
    "        items_df['item_id'] = items_df['product_id']\n",
    "        missing_columns.remove('item_id')\n",
    "    if 'item_name' in missing_columns and 'title' in items_df.columns:\n",
    "        items_df['item_name'] = items_df['title']\n",
    "        missing_columns.remove('item_name')\n",
    "    \n",
    "    # If still missing essential columns, create fallback data\n",
    "    if missing_columns:\n",
    "        print(f\"Still missing essential columns: {missing_columns}. Creating fallback data.\")\n",
    "        # Add missing columns with default values\n",
    "        for col in missing_columns:\n",
    "            if col == 'item_id':\n",
    "                items_df[col] = [f\"game_{i}\" for i in range(len(items_df))]\n",
    "            elif col == 'item_name':\n",
    "                items_df[col] = [f\"Game Title {i}\" for i in range(len(items_df))]\n",
    "            elif col == 'genres':\n",
    "                # Not essential, can skip\n",
    "                pass\n",
    "\n",
    "# Keep only available essential columns\n",
    "essential_item_columns = [col for col in essential_item_columns if col in items_df.columns]\n",
    "items_df = items_df[essential_item_columns]\n",
    "\n",
    "print(\"\\nCleaned reviews shape:\", reviews_df.shape)\n",
    "print(\"Cleaned items shape:\", items_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values and Outliers\n",
    "\n",
    "Let's handle missing values and outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values after column selection\n",
    "print(\"Missing values in reviews after column selection:\")\n",
    "print(reviews_df.isnull().sum())\n",
    "print(\"\\nMissing values in items after column selection:\")\n",
    "print(items_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in essential columns\n",
    "reviews_df = reviews_df.dropna(subset=['user_id', 'item_id'])\n",
    "items_df = items_df.dropna(subset=['item_id'])\n",
    "\n",
    "# Fill missing playtime values with 0\n",
    "if 'playtime_forever' in reviews_df.columns:\n",
    "    reviews_df['playtime_forever'] = reviews_df['playtime_forever'].fillna(0)\n",
    "    \n",
    "if 'playtime_2weeks' in reviews_df.columns:\n",
    "    reviews_df['playtime_2weeks'] = reviews_df['playtime_2weeks'].fillna(0)\n",
    "\n",
    "# Convert playtime columns to numeric if they aren't already\n",
    "reviews_df['playtime_forever'] = pd.to_numeric(reviews_df['playtime_forever'], errors='coerce').fillna(0).astype(int)\n",
    "if 'playtime_2weeks' in reviews_df.columns:\n",
    "    reviews_df['playtime_2weeks'] = pd.to_numeric(reviews_df['playtime_2weeks'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "print(f\"After handling missing values - reviews: {reviews_df.shape}, items: {items_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers in Playtime\n",
    "\n",
    "Let's examine and handle outliers in the playtime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in playtime_forever\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=reviews_df['playtime_forever'])\n",
    "plt.title('Boxplot of Playtime Forever')\n",
    "plt.xlabel('Playtime (minutes)')\n",
    "plt.show()\n",
    "\n",
    "# Calculate statistics\n",
    "playtime_stats = reviews_df['playtime_forever'].describe()\n",
    "print(\"Playtime statistics:\")\n",
    "print(playtime_stats)\n",
    "\n",
    "# Calculate IQR and upper limit for outliers\n",
    "Q1 = reviews_df['playtime_forever'].quantile(0.25)\n",
    "Q3 = reviews_df['playtime_forever'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_limit = Q3 + 3 * IQR  # Using 3 times IQR for a more relaxed threshold\n",
    "\n",
    "print(f\"\\nIQR: {IQR}\")\n",
    "print(f\"Upper limit for outliers: {upper_limit}\")\n",
    "print(f\"Number of outliers: {(reviews_df['playtime_forever'] > upper_limit).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap extreme playtime values\n",
    "cap_value = min(upper_limit, 10000)  # Cap at upper limit or 10000 minutes (166.7 hours), whichever is smaller\n",
    "print(f\"Capping playtime at {cap_value} minutes ({cap_value/60:.1f} hours)\")\n",
    "\n",
    "# Apply capping\n",
    "reviews_df_capped = reviews_df.copy()\n",
    "reviews_df_capped['playtime_forever'] = reviews_df_capped['playtime_forever'].clip(upper=cap_value)\n",
    "\n",
    "# Check the effect of capping\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(reviews_df['playtime_forever'], bins=50, log_scale=True)\n",
    "plt.title('Original Playtime Distribution (Log Scale)')\n",
    "plt.xlabel('Playtime (minutes)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(reviews_df_capped['playtime_forever'], bins=50, log_scale=True)\n",
    "plt.title('Capped Playtime Distribution (Log Scale)')\n",
    "plt.xlabel('Playtime (minutes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Use the capped data for further processing\n",
    "reviews_df = reviews_df_capped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Users and Games Based on Activity Thresholds\n",
    "\n",
    "We'll filter out users and games with minimal activity to ensure that our recommendation models have sufficient data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of games per user and users per game\n",
    "user_game_counts = reviews_df.groupby('user_id')['item_id'].count()\n",
    "game_user_counts = reviews_df.groupby('item_id')['user_id'].count()\n",
    "\n",
    "# Plot distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(user_game_counts, log_scale=True, bins=50)\n",
    "plt.title('Games per User (Log Scale)')\n",
    "plt.xlabel('Number of Games')\n",
    "plt.ylabel('Number of Users')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(game_user_counts, log_scale=True, bins=50)\n",
    "plt.title('Users per Game (Log Scale)')\n",
    "plt.xlabel('Number of Users')\n",
    "plt.ylabel('Number of Games')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define thresholds\n",
    "min_user_games = 5  # Minimum number of games per user\n",
    "min_game_users = 10  # Minimum number of users per game\n",
    "\n",
    "# Display statistics for different thresholds\n",
    "print(f\"Current data: {len(reviews_df)} interactions, {len(user_game_counts)} users, {len(game_user_counts)} games\")\n",
    "\n",
    "for user_threshold in [2, 5, 10]:\n",
    "    for game_threshold in [5, 10, 20]:\n",
    "        active_users = user_game_counts[user_game_counts >= user_threshold].index\n",
    "        popular_games = game_user_counts[game_user_counts >= game_threshold].index\n",
    "        filtered = reviews_df[\n",
    "            (reviews_df['user_id'].isin(active_users)) & \n",
    "            (reviews_df['item_id'].isin(popular_games))\n",
    "        ]\n",
    "        print(f\"Thresholds - Min games/user: {user_threshold}, Min users/game: {game_threshold} -> \"\n",
    "              f\"{len(filtered)} interactions, {filtered['user_id'].nunique()} users, {filtered['item_id'].nunique()} games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the selected thresholds\n",
    "print(f\"Applying thresholds - Min games per user: {min_user_games}, Min users per game: {min_game_users}\")\n",
    "\n",
    "# Get active users and popular games\n",
    "active_users = user_game_counts[user_game_counts >= min_user_games].index\n",
    "popular_games = game_user_counts[game_user_counts >= min_game_users].index\n",
    "\n",
    "# Filter the dataframe\n",
    "filtered_reviews = reviews_df[\n",
    "    (reviews_df['user_id'].isin(active_users)) & \n",
    "    (reviews_df['item_id'].isin(popular_games))\n",
    "]\n",
    "\n",
    "print(f\"Original data: {len(reviews_df)} interactions, {reviews_df['user_id'].nunique()} users, {reviews_df['item_id'].nunique()} games\")\n",
    "print(f\"Filtered data: {len(filtered_reviews)} interactions, {filtered_reviews['user_id'].nunique()} users, {filtered_reviews['item_id'].nunique()} games\")\n",
    "\n",
    "# Use the filtered data for further processing\n",
    "reviews_df = filtered_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Playtime Information\n",
    "\n",
    "Let's normalize playtime to a rating-like scale for better interpretability and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine playtime distribution after filtering\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(reviews_df['playtime_forever'], bins=50, log_scale=True)\n",
    "plt.title('Playtime Distribution after Filtering (Log Scale)')\n",
    "plt.xlabel('Playtime (minutes)')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Statistics of filtered playtime\n",
    "print(\"Playtime statistics after filtering:\")\n",
    "print(reviews_df['playtime_forever'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to normalize playtime to a scale from 0 to 5\n",
    "def normalize_playtime(playtime, max_value=None):\n",
    "    \"\"\"\n",
    "    Normalize playtime to a scale from 0 to 5.\n",
    "    \n",
    "    Args:\n",
    "        playtime: Raw playtime in minutes\n",
    "        max_value: Maximum value for normalization (default: None, will use empirical cap)\n",
    "        \n",
    "    Returns:\n",
    "        Normalized playtime as a float between 0 and 5\n",
    "    \"\"\"\n",
    "    if max_value is None:\n",
    "        # Use the 99th percentile as max_value to avoid extreme outliers affecting the normalization\n",
    "        max_value = np.percentile(playtime, 99)\n",
    "    \n",
    "    # Apply a log transformation to handle the skewed distribution\n",
    "    log_playtime = np.log1p(playtime)  # log(1+x) to handle zeros\n",
    "    log_max = np.log1p(max_value)\n",
    "    \n",
    "    # Normalize to 0-5 scale\n",
    "    normalized = 5 * log_playtime / log_max\n",
    "    \n",
    "    # Clip to ensure values are between 0 and 5\n",
    "    return np.clip(normalized, 0, 5)\n",
    "\n",
    "# Apply normalization\n",
    "max_playtime = np.percentile(reviews_df['playtime_forever'], 99)\n",
    "reviews_df['normalized_playtime'] = normalize_playtime(reviews_df['playtime_forever'], max_playtime)\n",
    "\n",
    "# Examine the normalized distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(reviews_df['normalized_playtime'], bins=50)\n",
    "plt.title('Normalized Playtime Distribution (0-5 Scale)')\n",
    "plt.xlabel('Normalized Playtime')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Statistics of normalized playtime\n",
    "print(\"Normalized playtime statistics:\")\n",
    "print(reviews_df['normalized_playtime'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create User-Item Interaction Matrix\n",
    "\n",
    "Now let's create the user-item interaction matrix that will be used for collaborative filtering and matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings between IDs and indices\n",
    "user_ids = reviews_df['user_id'].unique()\n",
    "item_ids = reviews_df['item_id'].unique()\n",
    "\n",
    "user_to_idx = {user: i for i, user in enumerate(user_ids)}\n",
    "item_to_idx = {item: i for i, item in enumerate(item_ids)}\n",
    "idx_to_user = {i: user for user, i in user_to_idx.items()}\n",
    "idx_to_item = {i: item for item, i in item_to_idx.items()}\n",
    "\n",
    "# Create interaction matrix using normalized playtime\n",
    "interaction_matrix = reviews_df.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='normalized_playtime',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "print(f\"Sparsity: {100.0 * (1 - np.count_nonzero(interaction_matrix) / interaction_matrix.size):.2f}%\")\n",
    "\n",
    "# Convert to sparse matrix for efficiency\n",
    "sparse_interaction_matrix = csr_matrix(interaction_matrix.values)\n",
    "\n",
    "# Create binary interaction matrix (played or not played)\n",
    "binary_interaction_matrix = (interaction_matrix > 0).astype(int)\n",
    "sparse_binary_matrix = csr_matrix(binary_interaction_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a sample of the interaction matrix to understand the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset for visualization\n",
    "n_sample = 50\n",
    "sample_users = np.random.choice(interaction_matrix.index, min(n_sample, len(interaction_matrix.index)), replace=False)\n",
    "sample_items = np.random.choice(interaction_matrix.columns, min(n_sample, len(interaction_matrix.columns)), replace=False)\n",
    "\n",
    "sample_matrix = interaction_matrix.loc[sample_users, sample_items]\n",
    "\n",
    "# Visualize the sample\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(sample_matrix, cmap='viridis')\n",
    "plt.title(f'Sample of User-Item Interaction Matrix ({n_sample}x{n_sample})')\n",
    "plt.xlabel('Item ID')\n",
    "plt.ylabel('User ID')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Sets\n",
    "\n",
    "To evaluate our recommendation models, we'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for train-test split at the user level\n",
    "def train_test_split_leave_n_out(df, n_test_items=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split interactions into train and test sets using leave-n-out strategy per user.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing user-item interactions\n",
    "        n_test_items: Number of items to leave out for each user\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_df: DataFrame with training interactions\n",
    "        test_df: DataFrame with test interactions\n",
    "    \"\"\"\n",
    "    train_df = pd.DataFrame(columns=df.columns)\n",
    "    test_df = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # Group by user_id\n",
    "    grouped = df.groupby('user_id')\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    for user_id, user_data in tqdm(grouped, desc=\"Splitting data\"):\n",
    "        # Ensure user has enough items to split\n",
    "        n_items = len(user_data)\n",
    "        if n_items <= n_test_items:\n",
    "            # If user doesn't have enough items, put all in training\n",
    "            train_df = pd.concat([train_df, user_data])\n",
    "            continue\n",
    "        \n",
    "        # Randomly select test items\n",
    "        test_indices = np.random.choice(user_data.index, n_test_items, replace=False)\n",
    "        test_items = user_data.loc[test_indices]\n",
    "        train_items = user_data.drop(test_indices)\n",
    "        \n",
    "        # Add to respective dataframes\n",
    "        train_df = pd.concat([train_df, train_items])\n",
    "        test_df = pd.concat([test_df, test_items])\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Apply the split\n",
    "train_df, test_df = train_test_split_leave_n_out(reviews_df, n_test_items=2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} interactions, {train_df['user_id'].nunique()} users, {train_df['item_id'].nunique()} items\")\n",
    "print(f\"Test set: {len(test_df)} interactions, {test_df['user_id'].nunique()} users, {test_df['item_id'].nunique()} items\")"
   ]
  },
  {
  "cells": [
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "Let's create interaction matrices for the training set."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Create interaction matrices for training data\n",
     "train_matrix = train_df.pivot_table(\n",
     "    index='user_id',\n",
     "    columns='item_id',\n",
     "    values='normalized_playtime',\n",
     "    fill_value=0\n",
     ")\n",
     "\n",
     "# Convert to sparse matrix for efficiency\n",
     "sparse_train_matrix = csr_matrix(train_matrix.values)\n",
     "\n",
     "# Create binary interaction matrix for training data\n",
     "binary_train_matrix = (train_matrix > 0).astype(int)\n",
     "sparse_binary_train_matrix = csr_matrix(binary_train_matrix.values)\n",
     "\n",
     "print(f\"Training interaction matrix shape: {train_matrix.shape}\")\n",
     "print(f\"Training matrix sparsity: {100.0 * (1 - np.count_nonzero(train_matrix) / train_matrix.size):.2f}%\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## Save Processed Data\n",
     "\n",
     "Now let's save all the processed data for use in subsequent notebooks."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Create output directory if it doesn't exist\n",
     "processed_dir = '../data/processed'\n",
     "os.makedirs(processed_dir, exist_ok=True)\n",
     "\n",
     "# Save DataFrames\n",
     "train_df.to_csv(f'{processed_dir}/train_interactions.csv', index=False)\n",
     "test_df.to_csv(f'{processed_dir}/test_interactions.csv', index=False)\n",
     "items_df.to_csv(f'{processed_dir}/items.csv', index=False)\n",
     "\n",
     "# Save interaction matrices\n",
     "train_matrix.to_csv(f'{processed_dir}/train_matrix.csv')\n",
     "binary_train_matrix.to_csv(f'{processed_dir}/binary_train_matrix.csv')\n",
     "\n",
     "# Save ID mappings\n",
     "id_mappings = {\n",
     "    'user_to_idx': user_to_idx,\n",
     "    'item_to_idx': item_to_idx,\n",
     "    'idx_to_user': idx_to_user,\n",
     "    'idx_to_item': idx_to_item\n",
     "}\n",
     "\n",
     "with open(f'{processed_dir}/id_mappings.pkl', 'wb') as f:\n",
     "    pickle.dump(id_mappings, f)\n",
     "\n",
     "# Save sparse matrices\n",
     "sparse_matrices = {\n",
     "    'sparse_train_matrix': sparse_train_matrix,\n",
     "    'sparse_binary_train_matrix': sparse_binary_train_matrix\n",
     "}\n",
     "\n",
     "with open(f'{processed_dir}/sparse_matrices.pkl', 'wb') as f:\n",
     "    pickle.dump(sparse_matrices, f)\n",
     "    \n",
     "print(f\"Processed data saved to {processed_dir}/\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## Preprocessing Summary\n",
     "\n",
     "Let's summarize the preprocessing steps we've performed:"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "In this notebook, we've completed the following preprocessing steps:\n",
     "\n",
     "1. **Data Cleaning and Initial Transformations**\n",
     "   - Selected essential columns: user_id, item_id, playtime_forever, item_name, and genres (when available)\n",
     "   - Handled missing values by dropping rows with missing user or item IDs\n",
     "   - Filled missing playtime values with 0\n",
     "\n",
     "2. **Outlier Handling**\n",
     "   - Identified outliers in playtime using IQR method\n",
     "   - Capped extremely high playtime values to reduce their impact on the models\n",
     "\n",
     "3. **Filtering**\n",
     "   - Applied minimum thresholds for user activity (>= 5 games) and game popularity (>= 10 users)\n",
     "   - Filtered out users and games with insufficient data for reliable recommendations\n",
     "\n",
     "4. **Playtime Normalization**\n",
     "   - Transformed playtime values to a 0-5 scale using a logarithmic transformation\n",
     "   - This preserves the relative differences in playtime while making values more comparable\n",
     "\n",
     "5. **Interaction Matrix Creation**\n",
     "   - Created user-item interaction matrices with normalized playtime values\n",
     "   - Generated binary versions of matrices (played/not played)\n",
     "   - Converted to sparse representations for efficient computation\n",
     "\n",
     "6. **Train-Test Splitting**\n",
     "   - Used leave-n-out strategy, holding out 2 items per user for testing\n",
     "   - Created train and test DataFrames and matrices\n",
     "\n",
     "7. **Data Saving**\n",
     "   - Saved all processed data for use in model development\n",
     "\n",
     "The processed data maintains important features from the original dataset while addressing issues like sparsity, outliers, and data format that could affect model performance. The normalization step is particularly important as it transforms playtime into a more interpretable scale while preserving the signal of user preference.\n",
     "\n",
     "In the next notebook, we'll implement and evaluate baseline recommendation models using this processed data."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Display final statistics for the processed data\n",
     "print(\"Final Dataset Statistics\")\n",
     "print(\"=========================\\n\")\n",
     "print(f\"Number of users: {train_df['user_id'].nunique()}\")\n",
     "print(f\"Number of items: {train_df['item_id'].nunique()}\")\n",
     "print(f\"Number of interactions: {len(train_df)}\")\n",
     "print(f\"Data density: {100.0 * np.count_nonzero(train_matrix) / train_matrix.size:.4f}%\")\n",
     "print(f\"Average items per user: {len(train_df) / train_df['user_id'].nunique():.2f}\")\n",
     "print(f\"Average users per item: {len(train_df) / train_df['item_id'].nunique():.2f}\")\n",
     "print(f\"Normalized playtime range: {train_df['normalized_playtime'].min():.2f} - {train_df['normalized_playtime'].max():.2f}\")"
    ]
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.10"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 4
  }
